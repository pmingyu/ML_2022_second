{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T15:38:55.985290Z",
     "start_time": "2022-11-25T15:38:55.982445Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T15:38:57.001352Z",
     "start_time": "2022-11-25T15:38:55.987676Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T15:38:57.028656Z",
     "start_time": "2022-11-25T15:38:57.003179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0153, -0.4431], grad_fn=<AddBackward0>), 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    # Forward Propagation\n",
    "    def forward(self, x):        \n",
    "        x = torch.Tensor(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    # Select best action and corresponding Q-value\n",
    "    def act(self, state):\n",
    "        pred = self.forward(state)\n",
    "        result = pred.squeeze().argmax().item()\n",
    "        return result\n",
    "\n",
    "net_test = QNet(4,2,64)\n",
    "state = env.reset()\n",
    "net_test.forward(env.reset()), net_test.act(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T15:38:57.040688Z",
     "start_time": "2022-11-25T15:38:57.030326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.05714364,  0.5655521 , -0.10201085, -0.9005526 ], dtype=float32), array([-0.04364025,  0.17305994,  0.04624137, -0.26197276], dtype=float32), array([ 0.11381183,  0.18757986, -0.20426363, -0.5935359 ], dtype=float32), array([ 0.04975789,  0.36928758, -0.0903945 , -0.58081794], dtype=float32), array([ 0.10621776,  0.37970385, -0.18782851, -0.8217558 ], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "Sequence = namedtuple(\"Sequence\",('state','action','reward','next_state','done'))\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity        \n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Sequence(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, batch_size)\n",
    "        return Sequence(*zip(*experiences))\n",
    "\n",
    "done = False\n",
    "mem_test = ReplayMemory(500)\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state,reward,done,_ = env.step(action)\n",
    "    mem_test.push(state,action,reward,next_state,done)\n",
    "    state = next_state\n",
    "    \n",
    "batch = mem_test.sample(5)\n",
    "print(batch.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T15:38:57.050542Z",
     "start_time": "2022-11-25T15:38:57.042651Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, GAMMA):\n",
    "    s = torch.FloatTensor(batch.state)\n",
    "    r = torch.Tensor(batch.reward)    \n",
    "    a = torch.Tensor(batch.action).to(torch.int64)    \n",
    "    next_s = torch.FloatTensor(batch.next_state)\n",
    "    m = 1-torch.Tensor(batch.done).to(torch.int64)\n",
    "\n",
    "    target = r + GAMMA * net.forward(next_s).amax(1) * m\n",
    "    target = target.view(1,-1)\n",
    "    pred = net.forward(s).gather(1,a.view(-1,1)).view(1,-1)    \n",
    "    \n",
    "    loss = (target-pred).pow(2) ## COMPLETE THIS LINE\n",
    "    return loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T15:43:35.341549Z",
     "start_time": "2022-11-25T15:38:57.053749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12.0 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-902397830f90>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  s = torch.FloatTensor(batch.state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 23.0 0.7754367595792627\n",
      "100 28.92 0.6083347399212544\n",
      "150 39.9 0.47819555615878423\n",
      "200 115.0 0.3768430579362991\n",
      "250 237.86 0.2979096529543844\n",
      "300 305.08 0.23643625534397691\n",
      "350 192.06 0.18856072514693178\n",
      "400 347.46 0.1512752247395143\n",
      "450 170.3 0.1222372478250084\n",
      "500 152.76 0.09962244866518184\n",
      "550 276.22 0.08201002537050635\n",
      "600 132.52 0.06829345631682802\n",
      "650 177.98 0.05761098159677035\n",
      "700 185.72 0.04929146191964895\n",
      "750 203.38 0.042812213480328834\n",
      "800 102.2 0.037766169722072154\n",
      "850 105.2 0.03383630689172928\n",
      "900 113.32 0.030775726642095037\n",
      "950 259.0 0.028392144347027016\n",
      "1000 136.68 0.026535808589112905\n"
     ]
    }
   ],
   "source": [
    "def dqn_replay_memory(GAMMA, EPS_START, EPS_END, EPS_DECAY, NUM_EPI, BATCH_SIZE):\n",
    "    # initialize replay memory\n",
    "    memory = ReplayMemory(1000) \n",
    "    # initialize Q-network\n",
    "    net = QNet(4,2,32)\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    # for episodes 1,M do\n",
    "    avg_rewards = deque(maxlen=50)\n",
    "\n",
    "    for epi in range(NUM_EPI+1):\n",
    "        # init sequence\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        reward_list = []\n",
    "\n",
    "        # iterate inside the sequence\n",
    "        # calculate EPS\n",
    "        EPS = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * epi / EPS_DECAY)\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "            # with prob EPS select random action\n",
    "                if np.random.random() < EPS:\n",
    "                    action = env.action_space.sample()\n",
    "                else: # otherwise select action w.r.t policy\n",
    "                    action = net(state).squeeze().argmax().item() # Complete this code\n",
    "                \n",
    "            # proceed\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward_list.append(reward)\n",
    "            \n",
    "            # store transition in memory\n",
    "            memory.push(state,action,reward,next_state,done) # Complete this code\n",
    "            \n",
    "            # sample random minibatch from memory\n",
    "            if len(memory.memory)<BATCH_SIZE:\n",
    "                continue\n",
    "            else:\n",
    "                batch = memory.sample(BATCH_SIZE) # Complete this code\n",
    "            # calculate loss\n",
    "            loss=calc_loss(batch, net, GAMMA)\n",
    "\n",
    "            # perform gradient descent step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update state\n",
    "            state = next_state\n",
    "            \n",
    "        # insert avg\n",
    "        avg_rewards.append(sum(reward_list))\n",
    "        if epi%50 == 0:\n",
    "            print(epi, np.mean(avg_rewards), EPS)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "discount_rate = 0.95\n",
    "eps_start = 0.99\n",
    "eps_end = 0.02\n",
    "eps_decay = 200\n",
    "num_epi = 1000\n",
    "batch_size =128\n",
    "result = dqn_replay_memory(   \n",
    "    discount_rate, eps_start, eps_end, eps_decay, num_epi, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T15:44:27.933771Z",
     "start_time": "2022-11-25T15:44:27.928497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test cartpole\n",
    "total_rewards = 0\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = result.act(state)    \n",
    "    new_state,reward,done,_ = env.step(action)\n",
    "    total_rewards += reward\n",
    "    state = new_state\n",
    "    env.render()\n",
    "    \n",
    "print(total_rewards)\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
