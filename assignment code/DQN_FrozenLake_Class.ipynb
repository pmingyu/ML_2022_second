{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Envrionment\n",
    "env = gym.make(\"FrozenLake-v0\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# one hot encoder\n",
    "# input: observation spaces from gym\n",
    "# ouput: one-hot encoded vector (torch tensor)\n",
    "def one_hot(i):\n",
    "    state = np.zeros(shape=(1,env.observation_space.n),dtype=int)\n",
    "    state[0][i] = 1 \n",
    "    return torch.Tensor(state) # pytorch compatible variables\n",
    "\n",
    "one_hot(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12,4) #\n",
    ")\n",
    "\n",
    "model(one_hot(3)).squeeze().argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 0.7992\n",
      "100 0.01 0.7231098839732764\n",
      "200 0.01 0.6542641445193258\n",
      "300 0.03 0.5919730600991551\n",
      "400 0.12 0.5356125760805872\n",
      "500 0.24 0.48461805273305963\n",
      "600 0.22 0.4384786084624098\n",
      "700 0.48 0.3967320016141349\n",
      "800 0.44 0.3589599995691724\n",
      "900 0.44 0.32478418873812753\n",
      "1000 0.63 0.2938621834769546\n",
      "1100 0.62 0.26588419594363666\n",
      "1200 0.64 0.24056993253144524\n",
      "1300 0.6 0.2176657857861265\n",
      "1400 0.68 0.19694229367462207\n",
      "1500 0.64 0.1781918407513601\n",
      "1600 0.62 0.16122657819156724\n",
      "1700 0.77 0.14587654185374446\n",
      "1800 0.74 0.13198794951737225\n",
      "1900 0.79 0.11942166023695906\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "2000 0.82 0.108051780377682\n"
     ]
    }
   ],
   "source": [
    "## Initialize DQN with pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque \n",
    "\n",
    "## deep learning architecture for DQN\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12,4) #\n",
    ")\n",
    "\n",
    "## define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "## hyper parameters\n",
    "discount_factor = 0.95\n",
    "epsilon = 0.6\n",
    "epsilon_decay_factor = 0.999\n",
    "num_episodes = 500\n",
    "\n",
    "def DQN(model, optimizer, discount_factor, epsilon, epsilon_decay_factor, num_episodes):\n",
    "    ## Initialize the Q-network\n",
    "    ## Iterate from 1...M episodes\n",
    "    avg_reward = deque(maxlen=100)\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes+1):\n",
    "        ## Reset the game, initialize the states\n",
    "        state = env.reset()\n",
    "        \n",
    "        ## repeat process until end of the epsidoes\n",
    "        done = False\n",
    "        epsilon = epsilon * epsilon_decay_factor\n",
    "\n",
    "        reward_list = []\n",
    "        while not done:\n",
    "            ## with prob. epsilon take random action\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else: #otherwise, select best action w.r.t Q-network\n",
    "                action = model(one_hot(state)).squeeze().argmax().item()\n",
    "            # execute action and observe new status, and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # calcualte target value(y)\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + discount_factor*model(one_hot(next_state)).squeeze().max()\n",
    "            # calcualte pred\n",
    "            pred = model(one_hot(state)).squeeze().max()\n",
    "            # loss function\n",
    "            loss = (target-pred).pow(2)\n",
    "\n",
    "            # perform a gradient descent\n",
    "            optimizer.zero_grad() # initialze the optimizer with zero\n",
    "            loss.backward() # calculate the gradient of loss function\n",
    "            optimizer.step() # we update the weight of the model using gradient\n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "\n",
    "            # put reward to list\n",
    "            reward_list.append(reward)\n",
    "\n",
    "            if episode == num_episodes:\n",
    "                env.render()\n",
    "\n",
    "        avg_reward.append(sum(reward_list))\n",
    "        if episode%100 == 0: # for every 100 steps, print some info\n",
    "            print(episode, np.mean(avg_reward), epsilon)\n",
    "\n",
    "\n",
    "DQN(model, optimizer, 0.95, 0.8, 0.999, 2000)\n",
    "        \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "3d6a3abdb0c77d29eb9ab0b8f6a3390bc4253a6b67d22ea3be770ab81032328f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
