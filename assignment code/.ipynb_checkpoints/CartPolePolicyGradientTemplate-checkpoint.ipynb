{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T14:07:26.967951Z",
     "start_time": "2022-11-25T14:07:26.960356Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T14:07:27.295599Z",
     "start_time": "2022-11-25T14:07:27.275468Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T14:09:32.248649Z",
     "start_time": "2022-11-25T14:09:32.229097Z"
    }
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        ## Here, we calculate discounts for instance [0.99^1, 0.99^2, 0.99^3, ..., 0.99^len(rewards)]\n",
    "        discounts = []\n",
    "        for k in range(len(rewards)+1):\n",
    "            discounts.append(gamma**k)\n",
    "        ## We calculate the return by sum(gamma[t] * reward[t]) \n",
    "        rt = []\n",
    "        for g,r in zip(discounts,rewards):\n",
    "            rt.append(g*r)\n",
    "        R = sum(rt)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(log_prob)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8:\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T14:09:32.550154Z",
     "start_time": "2022-11-25T14:09:32.539264Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 3000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1,\n",
    "    \"lr\": 1e-3,\n",
    "    # \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}\n",
    "\n",
    "policy_net = Policy(params['state_space'],params['action_space'],params['h_size'])\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=params['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T14:09:58.628427Z",
     "start_time": "2022-11-25T14:09:32.900816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 17.20\n",
      "Episode 200\tAverage Score: 19.32\n",
      "Episode 300\tAverage Score: 17.82\n",
      "Episode 400\tAverage Score: 17.13\n",
      "Episode 500\tAverage Score: 16.96\n",
      "Episode 600\tAverage Score: 15.35\n",
      "Episode 700\tAverage Score: 16.99\n",
      "Episode 800\tAverage Score: 15.82\n",
      "Episode 900\tAverage Score: 15.88\n",
      "Episode 1000\tAverage Score: 15.49\n",
      "Episode 1100\tAverage Score: 15.40\n",
      "Episode 1200\tAverage Score: 15.86\n",
      "Episode 1300\tAverage Score: 14.61\n",
      "Episode 1400\tAverage Score: 16.07\n",
      "Episode 1500\tAverage Score: 16.02\n",
      "Episode 1600\tAverage Score: 15.90\n",
      "Episode 1700\tAverage Score: 16.11\n",
      "Episode 1800\tAverage Score: 16.71\n",
      "Episode 1900\tAverage Score: 17.09\n",
      "Episode 2000\tAverage Score: 16.36\n",
      "Episode 2100\tAverage Score: 15.40\n",
      "Episode 2200\tAverage Score: 17.03\n",
      "Episode 2300\tAverage Score: 17.77\n",
      "Episode 2400\tAverage Score: 16.96\n",
      "Episode 2500\tAverage Score: 18.00\n",
      "Episode 2600\tAverage Score: 21.69\n",
      "Episode 2700\tAverage Score: 20.96\n",
      "Episode 2800\tAverage Score: 19.99\n",
      "Episode 2900\tAverage Score: 18.86\n",
      "Episode 3000\tAverage Score: 19.57\n"
     ]
    }
   ],
   "source": [
    "policy = reinforce(\n",
    "    policy_net, \n",
    "    optimizer, \n",
    "    params['n_training_episodes'],\n",
    "    params['max_t'],\n",
    "    params['gamma'],\n",
    "    100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T14:10:07.332706Z",
     "start_time": "2022-11-25T14:10:07.327871Z"
    }
   },
   "outputs": [],
   "source": [
    "total_rewards = 0\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action,_ = policy.act(state)    \n",
    "    new_state,reward,done,_ = env.step(action)\n",
    "    total_rewards += reward\n",
    "    state = new_state\n",
    "    env.render()\n",
    "    \n",
    "print(total_rewards)\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "25323bdd98f096afa12a857b694ef741ac0c02964572d0e876cbdb4a5f366d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
